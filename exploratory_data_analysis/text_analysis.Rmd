---
title: "Text Analysis Notebook"
---

In this notebook we analyse the content of the tweets. We want to understand which are the main topics discussed and their corresponding development over time. 

## Prepare Notebook

```{r, warning=FALSE,  message=FALSE}
knitr::opts_knit$set(warning=FALSE, message=FALSE)

library(igraph)
library(magrittr)
library(lubridate)
library(tidytext)
library(tidyverse)
library(tm)
library(topicmodels)
library(widyr)
library(wordcloud)

source(file = "../R/hashtags.R")
source(file = "../R/newtheme.R")
source(file = "../R/text_cleaning.R")
source(file = "../R/text_mining.R")
```

## Read Data

We read the pre-processed data obtanied from `R/data_processed.R`. 

```{r, read data}
tweets_text_df <- readRDS(file = "../data/tweets_text_df.rds")
hashtags_count_df <- readRDS(file = "../data/hashtags_count.rds")
```

## Tracked Hashtags

In the data gathering phase we tracked the following hashtags:

```{r}
twitter_terms <- c(
  "21N", 
  "#21N", 
  "#21NSomosTodos",
  "#Paro21N", 
  "#YoMarchoEste21",
  "#YoMarchoEl21",
  "#YoNoMarchoEste21",
  "#YoNoMarchoEl21",
  "#RazonesParaMarchar",
  "#RazonesParaNoMarchar",
  "#100RazonesParaMarchar", 
  "#100RazonesParaNoMarchar", 
  "#YoNoParo",
  "#YoParoEl21NSinMiedo", 
  "#Cacerolazo", 
  "#22N", 
  "#23N",
  "#ToqueDeQueda"
)
```

Let us see the top hastags appearing in the data:

```{r, fig.align="center"}
normalized_twitter_terms <- twitter_terms %>% 
  str_to_lower() %>% 
  str_remove_all(pattern = "#")

hashtags_count_df %>% 
  mutate(is_tracked = hashtag %in% normalized_twitter_terms) %>% 
  mutate(n = n /sum(n)) %>% 
  top_n(n = 30, wt = n) %>% 
  mutate(hashtag = reorder(hashtag, n)) %>%
  ggmyplot(aes(x = hashtag, y = n, fill = is_tracked)) +
  geom_col() +
  xlab(NULL) +
  coord_flip() +
ggtitle(label = "Top Hashtags Count (Share)") 
```

## Tweets Over Time

Let us see the time development of these hashtags over time (which has already been converted into Colombian local time): 

```{r, fig.align="center"}
tweets_text_df %>% 
  count(created_at_round_hour) %>% 
  ggmyplot(mapping = aes(x = created_at_round_hour, y = n)) +
  geom_line(color = PALETTE_SET1[2]) +
  labs(title = "Tweets Over Time")
```

We see a remarkable peak. Let us find the time it happened:

```{r}
tweets_text_df %>% 
  count(created_at_round_hour) %>% 
  arrange(-n) %>% 
  head(5)
```

The most active hours where on the 21st of November between 20:00 and 23:00. There were many events around this time period which could have contributed to this: cacerolazo and the media spread on chaos in Bogota and Cali. We need to understand this better.

Let us see the organic vs retweets mentions: 

```{r, fig.align="center"}
tweets_text_df %>% 
  count(created_at_round_hour, is_retweet) %>% 
  ggmyplot(mapping = aes(x = created_at_round_hour, y = n, color = is_retweet)) +
  geom_line() +
  labs(title = "Tweets Over Time")
```

## Top Hashtags

Next we analyze the time development of the most popular hashtags:

```{r, fig.align="center"}
top_hashtags <- hashtags_count_df %>% head(3) %>% pull(hashtag)
top_hashtags <- str_c("#", top_hashtags)

top_hashtags %>% map_df(
  .f = ~ get_text_over_time(tweets_text_df = tweets_text_df, txt_pattern = .x, no_hashtag = FALSE) %>% 
    mutate(hashtag = .x)
  ) %>% 
  ggmyplot(mapping = aes(x = created_at_round_hour, y =n, color = hashtag)) + 
  geom_line() +
  labs(title = "Top Hashtags Count")
```

We clearly see two clusters which define the two prnciipal content of the tweets:

  - The protests of the 21N.
  - The cacerolazo. 

## Word Count

Next we do some initial descriptive statistics on the words used in the tweets (removing the hashtags). 

```{r}
stopwords_df <- tibble(word = stopwords::stopwords(language = "es"))

words_df <- tweets_text_df %>% 
  unnest_tokens(output = word, input = clean_text_no_hashtag) %>% 
  anti_join(y = stopwords_df, by = "word")

word_count <- words_df %>% 
  count(word) %>% 
  arrange(- n)
```

```{r, fig.align="center"}
word_count %>% 
  mutate(word = reorder(word, n)) %>%
  head(35) %>% 
  ggmyplot(mapping = aes(x = word, y = n)) +
  geom_col(fill = PALETTE_SET1[2]) +
  coord_flip() +
  labs(title = "Word Count (No Hashtags)")
```

Let us visualize this as a word cloud:

```{r, fig.align="center", fig.width=6}
wordcloud(
  words = head(word_count, 1000)$word, 
  freq = head(word_count, 1000)$n, 
  min.freq = 10000, 
  colors = brewer.pal(8, "Dark2"))
```

Let us now do a similar word count but restricting ourselves to tweets containing a specific word defining a topic.

- Marcha (Protest)

```{r, fig.align="center"}
input_word <- "marcha"

relative_word_count <- get_relatve_word_count(
  tweets_text_df = tweets_text_df, 
  stopwords_df = stopwords_df, 
  input_word = input_word 
)

relative_word_count %>% 
  mutate(word = reorder(word, n)) %>%
  head(35) %>% 
  ggmyplot(mapping = aes(x = word, y = n)) +
  geom_col(fill = PALETTE_SET1[2]) +
  coord_flip() +
  labs(title = glue::glue("Relative Word Count - {input_word}"))
```

Let us see the time development of some of the top words of this ranking:

```{r, fig.align="center"}
words_vect <- c(
  "marcha", 
  "gobierno",
  "ivanduque", 
  "colombia",
  "protesta", 
  "esmad", 
  "paro", 
  "pacífica"
)

words_vect %>% map_df(
  .f = ~ get_text_over_time(tweets_text_df = tweets_text_df, txt_pattern = .x, no_hashtag = TRUE) %>% 
    mutate(word = .x)
  ) %>% 
  ggmyplot(mapping = aes(x = created_at_round_hour, y = n, color = word)) + 
  geom_line() + 
  labs(title = "Related Words - Marcha")
```

Here are some observations:

  - It is interesting that the word "gobierno" (government) is more present surin the cacerolazo than during the protest period itself. 
  - This is also the case for the word "pacífica" (peaceful). 
  
- Cacerolazo

```{r, fig.align="center"}
input_word <- "cacerolazo"

relative_word_count <- get_relatve_word_count(
  tweets_text_df = tweets_text_df, 
  stopwords_df = stopwords_df, 
  input_word = input_word 
)

relative_word_count %>% 
  mutate(word = reorder(word, n)) %>%
  head(35) %>% 
  ggmyplot(mapping = aes(x = word, y = n)) +
  geom_col(fill = PALETTE_SET1[2]) +
  coord_flip() +
  labs(title = glue::glue("Relative Word Count - {input_word}"))
```

```{r, fig.align="center"}
words_vect <- c(
  "cacerolazo", 
  "gobierno",
  "violencia", 
  "miedo", 
  "vándalos"
)

words_vect %>% map_df(
  .f = ~ get_text_over_time(tweets_text_df = tweets_text_df, txt_pattern = .x, no_hashtag = TRUE) %>% 
    mutate(word = .x)
  ) %>% 
  ggmyplot(mapping = aes(x = created_at_round_hour, y = n, color = word)) + 
  geom_line() +
  labs(title = "Related Words - Cacerolazo")
```

Here are some observations:

  - The high peak is actually somehow split in two topics: cacerolazo and vándalos (vandals). 
  - The word "miedo" (fear) also appear as a relevant term, which might be related with the mentions about vandals. 

Let us see the mentions of Colombian cities:

```{r, fig.align="center"}
words_vect <- c(
  "barranquilla",
  "bogotá", 
  "cali", 
  "medellín"
)

words_vect %>% map_df(
  .f = ~ get_text_over_time(tweets_text_df = tweets_text_df, txt_pattern = .x, no_hashtag = TRUE) %>% 
    mutate(word = .x)
  ) %>% 
  ggmyplot(mapping = aes(x = created_at_round_hour, y = n, color = word)) + 
  geom_line() +
  labs(title = "Top Colombian Cities")
```

Bogota is clearly dominating. 

## Topic Modeling

### LDA

```{r, eval=FALSE}
# Create a Document-Tem-Matrix
tweets_text_dtm <- tweets_text_df %>% 
  select(clean_text_no_hashtag) %>% 
  rowid_to_column() %>% 
  unnest_tokens(input = clean_text_no_hashtag, output = "word") %>% 
  anti_join(y = stopwords_df, by = "word") %>% 
  count(rowid, word) %>% 
  cast_dtm(document = rowid, term = word, value = n) 

tweets_text_dtm
```

```{r, eval=FALSE}
# Train model.
lda_model <- LDA(x = tweets_text_dtm, k = 3, control = list(seed = 42))

topics_df <- tidy(x = lda_model, matrix ="beta")
```

```{r, eval=FALSE}
# Plot top terms per tocpic. 
topic_top_terms <- topics_df %>%
  group_by(topic) %>%
  top_n(30, beta) %>%
  ungroup() %>%
  arrange(topic, -beta)

topic_top_terms %>%
  mutate(term = reorder_within(term, beta, topic)) %>%
  ggmyplot(mapping = aes(term, beta, fill = factor(topic))) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  coord_flip() +
  scale_x_reordered()
```

Topic modeling (in my experience) does not work very well with social media data.

### Text Networks

Count the noumber of pairwise ocurrence per post.

```{r}
abs_pairwise_count_df <- tweets_text_df %>% 
  select(clean_text) %>% 
  rowid_to_column() %>% 
  unnest_tokens(output = word, input = clean_text) %>% 
  anti_join(y = stopwords_df, by = "word") %>% 
  pairwise_count(item = word, feature = rowid) %>% 
  filter(item1 < item2) %>% arrange(item1, item2) %>% 
  rename(weight = n) %>% 
  arrange(- weight)
```

Define a text graph. We wet a threshold to control noise. 

```{r}
weight_threshold <- 3000

abs_pairwise_graph <- graph_from_data_frame(
  d = filter(abs_pairwise_count_df, weight > weight_threshold), 
  directed = FALSE
)
```

We just consider the biggest connected component. 

```{r}
E(abs_pairwise_graph)$weight_log <- log(E(abs_pairwise_graph)$weight)

V(abs_pairwise_graph)$component <- clusters(graph = abs_pairwise_graph)$membership

V(abs_pairwise_graph)$degree <- strength(graph = abs_pairwise_graph)

mask_top <- which(V(abs_pairwise_graph)$component == which.max(clusters(graph = abs_pairwise_graph)$csize))

top_abs_pairwise_graph <- induced_subgraph(
  graph = abs_pairwise_graph,
  vids = mask_top
)
```


Run community detection algorithm to detect topics. 

```{r}
louvain_obj <- cluster_louvain(
  graph = top_abs_pairwise_graph, 
  weights = E(top_abs_pairwise_graph)$weight
)

V(top_abs_pairwise_graph)$membership <- membership(communities = louvain_obj)   

length(unique(V(top_abs_pairwise_graph)$membership))
```

Plot graph:

```{r}
pdf("../images/abs_count_graph.pdf") 

# Compute the weight shares.
E(top_abs_pairwise_graph)$width <- E(top_abs_pairwise_graph)$weight_log/max(E(top_abs_pairwise_graph)$weight_log)

 plot(
  top_abs_pairwise_graph, 
  vertex.color = V(top_abs_pairwise_graph)$membership,
  vertex.frame.color = V(top_abs_pairwise_graph)$membership,
  # Scale node size by degree.
  vertex.size = V(top_abs_pairwise_graph)$degree / 1E6,
  vertex.label.color = "black", 
  vertex.label.cex = 0.12, 
  vertex.label.dist = 0.12,
  edge.color = "gray", 
  # Set edge width proportional to the weight relative value.
  edge.width = (0.01)*E(top_abs_pairwise_graph)$width ,
  main = '21N - Twitter', 
  sub = "Word Network - Topics", 
  alpha = 50, 
  palette = c(PALETTE_SET1, "deepskyblue")
)

dev.off()
```

Get topic summary. 

```{r, fig.align="center", fig.width=8}
membership_df <- tibble(
  word = names(V(top_abs_pairwise_graph)),
  cluster = str_c("topic_cluster_", as.character(V(top_abs_pairwise_graph)$membership)),
  degree = V(top_abs_pairwise_graph)$degree
)

membership_df %>%
  pull(cluster) %>% 
  unique() %>% 
  sort() %>% 
  map_df(.f = function(cluster_id) {
    
    membership_df %>% 
      filter(cluster == cluster_id) %>% 
      head(15)
  }) %>% 
  mutate(word = reorder(word, degree)) %>%
  ggmyplot(mapping = aes(x = word, y = degree, fill = as.character(cluster))) +
  geom_col(color = "black") +
  coord_flip() +
  facet_wrap(facets = ~ cluster, scales = "free") +
  scale_fill_brewer(palette = "Set3")
```



